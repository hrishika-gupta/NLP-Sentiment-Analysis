# -*- coding: utf-8 -*-
"""Group 08 - NLP Project Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d8mt1DN88DfE6_UmZG6fHkmXQMkVNIH8

# <font color='Blue'> **NLP END Term Project **</font>

***

# <font color='Blue'>Natural Language Processig for *"Big Basket"* Google App REVIEWs

## <font color='Blue'>Installing necessary Module
"""

pip install textblob

pip install wordcloud

pip install nrclex

pip install vaderSentiment

"""## <font color='Blue'>1. Importing Libraries"""

import re
import numpy as np
import pandas as pd
import csv
import string
from textblob import TextBlob

# plotting
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

# nltk
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import TweetTokenizer
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from nltk import PorterStemmer

# sklearna
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, classification_report

"""## <font color='Blue'>2. Reading the data file"""

Rfile = pd.read_excel("Big Basket App REVIEWs.xlsx")
Rfile

Rdata = pd.DataFrame(Rfile[['REVIEW']])
Rdata.info()

print(Rdata)

"""## <font color='Blue'> 3. Feature Engineering

### <font color='Blue'> 3.1 Sentiment Column

#### <font color='Blue'> Applying Sentiment Analysis using polarity scores

Here, we have split the entire text data into Positive, Negative and Neutral.

Positive is represented by 3; polarity > 0.5

Neutral is represented by 2; polarity between -0.5 and +0.5

Negative is represented by 1; polarity < 0.5
"""

# Function to get sentiment polarity
def sentiment_polarity(text):
    analysis = TextBlob(text)
    polarity = analysis.sentiment.polarity
    return polarity

# Ensuring all entries are strings
Rdata['REVIEW'] = Rdata['REVIEW'].astype(str)

# Applying the sentiment analysis function to the 'REVIEW' column
Rdata['Polarity'] = Rdata['REVIEW'].apply(sentiment_polarity)

# Function to get sentiment polarity
def sentiment_polarity(text):
    analysis = TextBlob(text)
    polarity = analysis.sentiment.polarity
    return polarity

# Applying the sentiment analysis function to the 'REVIEW' column
Rdata['Polarity'] = Rdata['REVIEW'].apply(sentiment_polarity)

# Defining function to categorize polarity scores
def categorize_sentiment(polarity):
    if polarity > 0.5:
        return 3  # Positive
    elif -0.5 <= polarity <= 0.5:
        return 2  # Neutral
    else:
        return 1  # Negative

# Applying sentiment categorization to create a new column 'Sentiment'
Rdata['Sentiment'] = Rdata['Polarity'].apply(categorize_sentiment)


# Print the first few rows of the updated DataFrame
print(Rdata.head())

"""## <font color='Blue'> 4. Data Cleaning and Preprocessing

### <font color='Blue'> 4.1 Converting text into lower case
"""

Rdata['REVIEW']=Rdata['REVIEW'].str.lower()
Rdata['REVIEW'].head()

"""### <font color='Blue'> 4.2 Removing Stop Words"""

stopwords = nltk.corpus.stopwords.words('english')
print("Stopwords in nltk :", stopwords, end = " ")

def clean_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in stopwords])
Rdata['REVIEW'] = Rdata['REVIEW'].apply(lambda text: clean_stopwords(text))
Rdata['REVIEW'].head()

"""### <font color='Blue'> 4.3 Removing special characters"""

def clean_specialCharacters(data):
    return re.sub(r'((www\.[^\s]+)|(https?://[^\s]+)|(<br />))|/', ' ', data)
Rdata['REVIEW'] = Rdata['REVIEW'].apply(lambda x: clean_specialCharacters(x))
Rdata['REVIEW'].head()

"""### <font color='Blue'> 4.4 Removing Numeric numbers"""

def clean_numbers(data):
    return re.sub('[0-9]+', '', data)
Rdata['REVIEW'] = Rdata['REVIEW'].apply(lambda x: clean_numbers(x))
Rdata['REVIEW'].tail()

"""### <font color='Blue'> 4.5 Removing punctuations"""

english_punctuations = string.punctuation
punctuations_list = english_punctuations
def clean_punctuations(text):
    translator = str.maketrans('', '', punctuations_list)
    return text.translate(translator)
Rdata['REVIEW']= Rdata['REVIEW'].apply(lambda x: clean_punctuations(x))
Rdata['REVIEW'].head()

"""### <font color='Blue'> 4.6 Removing repeating characters in single REVIEW"""

def clean_repeating_char(text):
    return re.sub(r'(.)1+', r'1', text)
Rdata['REVIEW'] = Rdata['REVIEW'].apply(lambda x: clean_repeating_char(x))
Rdata['REVIEW'].head()

"""##### <font color='Black'> Selecting target columns from dataframe for future Analysis"""

Rdata2 = Rdata[['REVIEW']]

"""### <font color='Blue'> 4.7 Tokenization"""

Rdata['REVIEW'] = Rdata['REVIEW'].apply(nltk.word_tokenize)
Rdata['REVIEW'].head()

"""### <font color='Blue'> 4.8 Stemming using Porter Stemmer"""

st = PorterStemmer()
def stemming_on_text(data):
    text = [st.stem(word) for word in data]
    return data
Rdata['REVIEW']= Rdata['REVIEW'].apply(lambda x: stemming_on_text(x))
Rdata['REVIEW'].head()

"""### <font color='Blue'> 4.9 Lemmatizing"""

lm = WordNetLemmatizer()
def lemmatizer_on_text(data):
    text = [lm.lemmatize(word) for word in data]
    return data

Rdata['REVIEW'] = Rdata['REVIEW'].apply(lambda x: lemmatizer_on_text(x))
Rdata['REVIEW'].tail()

"""## <font color='Blue'> 5. Text Preprocessing

### <font color='Blue'> 5.1 Separating input feature and label
"""

X=Rdata.REVIEW
y=Rdata.Sentiment

"""## <font color='Blue'> 5.2 Text Vectorization

#### <font color='Blue'> Tokenizinge the REVIEW text and converting data in matrix format
"""

# Converting the list of lists to a list of strings
preprocessed_X = [' '.join(document) for document in X]

# Initializing the vectorizer
vectorizer = CountVectorizer(stop_words='english')

# Transforming the preprocessed data
X_vec = vectorizer.fit_transform(preprocessed_X)
X_vec = X_vec.todense()
X_vec

"""#### <font color='Blue'>Transforming data by applying term frequency inverse document frequency (TF-IDF)"""

# Converting the list of lists to a list of strings
preprocessed_X = [' '.join(document) for document in X]

# Initializing the vectorizer
vectorizer = TfidfVectorizer(stop_words='english')

# Transforming the preprocessed data using TF-IDF
X_tfidf = vectorizer.fit_transform(preprocessed_X)

# Converting the sparse matrix to a dense matrix (for demonstration purposes)
X_tfidf_dense = X_tfidf.todense()

# Displaying the dense TF-IDF matrix (replace this with your preferred display method)
print(X_tfidf_dense)

"""## <font color='Blue'> 6. MODELLING

#### <font color='Blue'>Splitting data into Train and Test Subset with in 80% - 20% ratio
"""

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size = 0.20, random_state = 10)

"""### <font color='Blue'>6.1 Naive Bayes - Model"""

# Training the model
clf_NB = MultinomialNB()
clf_NB.fit(X_train, y_train)

"""#### <font color='Blue'>Predicting the Test  results"""

y_pred_1 = clf_NB.predict(X_test)
print(classification_report(y_test, y_pred_1, zero_division=1))

"""#### <font color='Blue'> Confusion Matrix"""

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred_1)
cm

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_pred = clf_NB.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

class_labels = ['1', '2', '3']
# Ploting confusion matrix using seaborn and matplotlib
plt.figure(figsize=(4, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False, xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""### <font color='Blue'>6.2 Logistic Regression"""

# Training the model
clf_LR = LogisticRegression()
clf_LR.fit(X_train, y_train)

"""#### <font color='Blue'>Predicting the Test  results"""

y_pred_2 = clf_LR.predict(X_test)
print(classification_report(y_test, y_pred_2, zero_division=1))

"""#### <font color='Blue'>Confusion Matrix"""

from sklearn.metrics import confusion_matrix
cm2 = confusion_matrix(y_test, y_pred_2)
cm2

y_pred2 = clf_LR.predict(X_test)
cm2 = confusion_matrix(y_test, y_pred2)
class_labels = ['1', '2', '3']
# Plot confusion matrix using seaborn and matplotlib
plt.figure(figsize=(4, 4))
sns.heatmap(cm2, annot=True, fmt='d', cmap='Greens', cbar=False, xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""### <font color='Blue'>6.3 Random Forest"""

# Training the model
from sklearn.ensemble import RandomForestClassifier
clf_RF = RandomForestClassifier()
clf_RF.fit(X_train, y_train)

"""#### <font color='Blue'>Predicting the Test  results"""

y_pred_3 = clf_RF.predict(X_test)
print(classification_report(y_test, y_pred_3, zero_division=1))

"""#### <font color='Blue'>Confusion Matrix"""

from sklearn.metrics import confusion_matrix
cm3 = confusion_matrix(y_test, y_pred_3)
cm3

y_pred3 = clf_RF.predict(X_test)
cm3 = confusion_matrix(y_test, y_pred3)
class_labels = ['1', '2', '3']
# Plot confusion matrix using seaborn and matplotlib
plt.figure(figsize=(4, 4))
sns.heatmap(cm3, annot=True, fmt='d', cmap='Greens', cbar=False,xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""### <font color='Blue'> 6.4 SVM (Support Vector Machines)"""

from sklearn.svm import SVC

# Creating an instance of SVM classifier
clf_SVM = SVC()

# Training the SVM classifier on the training data
clf_SVM.fit(X_train, y_train)

"""#### <font color='Blue'>Predicting the Test results"""

y_pred_SVM = clf_SVM.predict(X_test)

# Generating the classification report
report = classification_report(y_test, y_pred_SVM)
print(report)

"""#### <font color='Blue'>Confusion Matrix"""

from sklearn.metrics import confusion_matrix
cm4 = confusion_matrix(y_test, y_pred_SVM)
cm4

y_pred_SVM = clf_RF.predict(X_test)
cm4 = confusion_matrix(y_test, y_pred_SVM)
class_labels = ['1', '2', '3']
# Ploting confusion matrix using seaborn and matplotlib
plt.figure(figsize=(4, 4))
sns.heatmap(cm3, annot=True, fmt='d', cmap='Greens', cbar=False,xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""### <font color='Blue'>Comparing Accuracy

1. **Naive Bayes:**
   - Accuracy: 87%


2. **Logistic Regression (LR):**
   - Accuracy: 92%

3. **Random Forest (RF):**
   - Accuracy: 93%

4. **Support Vector Machine (SVM):**
   - Accuracy: 93%


- **Logistic Regression (LR)** seems to have the second-highest accuracy among all the models.
- **Naive Bayes** has a lower accuracy than LR but might still have strengths in certain scenarios, especially if the dataset is imbalanced.
- **Random Forest (RF)** and **SVM** have similar accuracy values having the highest accuracy among all the models.

## <font color='Blue'>7. Sentiment Analysis

### <font color='Blue'>7.1 Sentiment Analysis using NRClex (National Research Council Canada) on the non-vectorized data
"""

from nrclex import NRCLex

sentiments = []

for REVIEW in Rdata2['REVIEW']:
    emotion_analyzer = NRCLex(REVIEW)
    emotions = emotion_analyzer.affect_frequencies
    sentiments.append(emotions)

# Creating a new DataFrame with sentiment information
sentiments_df = pd.DataFrame(sentiments)
# Combining with the original DataFrame
result_df = pd.concat([Rdata2, sentiments_df], axis=1)

result_df

"""#### <font color='Blue'>Determining dominant emotions"""

# Determining dominant emotions
dominant_emotions = result_df[['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']].idxmax(axis=1)
result_df['Dominant Emotion'] = dominant_emotions

# Printing the DataFrame with dominant emotions
print(result_df[['REVIEW', 'Dominant Emotion']])

"""### <font color='Blue'>7.2 Sentiment Analysis using SentiWordNet lexicon"""

nltk.download('sentiwordnet')

from nltk.corpus import sentiwordnet as swn

# Initializing an empty list to store sentiment scores
sentiments = []

# Function to calculate a single sentiment score for a text
def calculate_overall_sentiment_score(text):
    overall_score = 0

    for word in text:
        synsets = list(swn.senti_synsets(word))

        for synset in synsets:
            overall_score += synset.pos_score() - synset.neg_score()

    return overall_score

# Applying sentiment analysis to each review in the 'review' column
for REVIEW in Rdata2['REVIEW']:
    sentiment_score = calculate_overall_sentiment_score(REVIEW)
    sentiments.append(sentiment_score)

# Creating a new DataFrame with sentiment information
sentiments_df = pd.DataFrame(sentiments, columns=['Overall_Score'])

# Combining with the original DataFrame
Rdata3 = pd.concat([Rdata2, sentiments_df], axis=1)

# Creating a new column 'swn_analysis' based on the overall score
def categorize_swn_sentiment(score):
    if score > 0:
        return 'positive'
    elif score < 0:
        return 'negative'
    else:
        return 'neutral'

Rdata3['swn_analysis'] = Rdata3['Overall_Score'].apply(categorize_swn_sentiment)

# Printing the updated DataFrame
print(Rdata3[['REVIEW', 'swn_analysis']])

"""### <font color='Blue'> 7.3 Sentiment Analysis using VADER (Valence Aware Dictionary and Sentiment Reasoner)"""

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Initializing an empty list to store sentiment scores
vader_sentiments = []

# Creating a Vader sentiment analyzer
vader_analyzer = SentimentIntensityAnalyzer()

# Function to calculate Vader sentiment scores for a text
def calculate_vader_sentiment_scores(text):
    sentiment_scores = vader_analyzer.polarity_scores(text)
    return sentiment_scores['compound']

# Applying Vader sentiment analysis to each review in the 'REVIEW' column
for REVIEW in Rdata2['REVIEW']:
    vader_sentiment_score = calculate_vader_sentiment_scores(REVIEW)
    vader_sentiments.append(vader_sentiment_score)

# Creating a new DataFrame with Vader sentiment information
vader_sentiments_df = pd.DataFrame(vader_sentiments, columns=['Vader_Score'])

# Combining with the original DataFrame
Rdata4 = pd.concat([Rdata2, vader_sentiments_df], axis=1)

# Creating a new column 'vader_analysis' based on the Vader score
def categorize_vader_sentiment(score):
    if score > 0.05:
        return 'positive'
    elif score < -0.05:
        return 'negative'
    else:
        return 'neutral'

Rdata4['vader_analysis'] = Rdata4['Vader_Score'].apply(categorize_vader_sentiment)

# Printing the updated DataFrame with Vader analysis
print(Rdata4[['REVIEW','vader_analysis']])

"""### <font color='Blue'> Comparing results of Sentiment Analysis"""

# Extracting the relevant columns from each DataFrame
result_df_subset = result_df[['Dominant Emotion']]
Rdata3_subset = Rdata3[['swn_analysis']]
Rdata4_subset = Rdata4[['vader_analysis']]

# Concatenating the subsets along the columns
concatenated_df = pd.concat([result_df_subset, Rdata3_subset, Rdata4_subset], axis=1)

# Printing the concatenated DataFrame
print(concatenated_df)

"""## <font color='Blue'>8. Topic Modeling"""

from sklearn.decomposition import LatentDirichletAllocation

"""### <font color='Blue'> 8.1 LDA (Latent Dirichlet Allocation) Model"""

number_of_topics = 5
lda = LatentDirichletAllocation(n_components=number_of_topics, random_state=42)
lda.fit( np.asarray(X_vec))

# Displaying topics and top words for each topic
feature_names = vectorizer.get_feature_names_out()
num_words = 10

for idx, topic in enumerate(lda.components_):
    top_words_idx = topic.argsort()[-num_words:][::-1]
    top_words = [feature_names[i] for i in top_words_idx]
    print(f"Topic {idx+1}: {', '.join(top_words)}")

"""#### <font color='Blue'> 8.1.1 Word Clouds for 5 Topics"""

# Word Clouds for Topics
for idx, topic in enumerate(lda.components_):
    plt.figure(figsize=(8, 8))
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(zip(feature_names, topic)))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f"Topic {idx+1} - Word Cloud")
    plt.axis('off')
    plt.show()

"""##  <font color='Blue'> 9. Exploratory Analysis and Visualization

### <font color='Blue'> 9.1 Bar graphs for Negative, Positive and Neutral Sentiments
"""

# Creating a bar chart
ax = Rdata['Sentiment'].value_counts().sort_index().plot(kind='bar', title='Distribution of data', legend=False, color=['red','blue','green'])

# Setting x-axis tick labels
ax.set_xticklabels(['Negative', 'Neutral', 'Positive'], rotation=0)
plt.title('Distribution of Positive, Negative and Nuetral REVIEWs')
plt.xlabel('Sentiment')
plt.ylabel('Number of Text')
plt.figure(figsize=(14, 14))

# Displaying the bar chart
plt.show()

"""### <font color='Blue'>9.2 Bar graphs for sentiments and emotions using NRClex on the non-vectorized data"""

sentiments = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']
sentiment_counts = result_df[sentiments].sum()

# Creating a bar graph
plt.figure(figsize=(8, 6))
plt.bar(sentiments, sentiment_counts, color='pink')
plt.xlabel('Sentiments')
plt.ylabel('Count')
plt.title('Sentiment Distribution')
plt.xticks(rotation=45)
plt.tight_layout()

# Showing the plot
plt.show()

sentiment_counts

"""### <font color='Blue'>9.3 Bar Graph for Most common words

#### <font color='Blue'> 9.3.1 Most Common Words in Positive reviews
"""

positive_REVIEWs = Rdata[Rdata['Sentiment'] == 3]['REVIEW']

# Flattenning the list of lists into a single list of strings
positive_REVIEWs_flattened = [text for sublist in positive_REVIEWs for text in sublist]

# Converting the list of strings to a single string
positive_text = ' '.join(positive_REVIEWs_flattened)

# Creating the CountVectorizer
vectorizer = CountVectorizer(stop_words='english')

# Fitting and transforming the data
X11 = vectorizer.fit_transform([positive_text])

# Converting the result to a DataFrame
word_freq = pd.DataFrame(X11.toarray(), columns=vectorizer.get_feature_names_out())

# Getting the most common words
most_common_words = word_freq.sum().sort_values(ascending=False)[:20]

plt.figure(figsize=(10, 6))
most_common_words.plot(kind='bar', color='Green')
plt.title('Most Common Words in Postive REVIEWs')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

"""#### <font color='Blue'> 9.3.2 Most Common Words in Neutral reviews"""

neutral_REVIEWs = Rdata[Rdata['Sentiment'] == 2]['REVIEW']

# Flattenning the list of lists into a single list of strings
neutral_REVIEWs_flattened = [text for sublist in neutral_REVIEWs for text in sublist]

# Converting the list of strings to a single string
neutral_text = ' '.join(neutral_REVIEWs_flattened)

# Creating the CountVectorizer
vectorizer = CountVectorizer(stop_words='english')

# Fitting and transforming the data
X12 = vectorizer.fit_transform([neutral_text])

# Converting the result to a DataFrame
word_freq = pd.DataFrame(X12.toarray(), columns=vectorizer.get_feature_names_out())

# Getting the most common words
most_common_words = word_freq.sum().sort_values(ascending=False)[:20]

plt.figure(figsize=(10, 6))
most_common_words.plot(kind='bar', color='Blue')
plt.title('Most Common Words in Neutral REVIEWs')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

"""#### <font color='Blue'> 9.3.3 Most Common Words in Negative reviews"""

negative_REVIEWs = Rdata[Rdata['Sentiment'] == 1]['REVIEW']

# Flattenning the list of lists into a single list of strings
negative_REVIEWs_flattened = [text for sublist in negative_REVIEWs for text in sublist]

# Converting the list of strings to a single string
negative_text = ' '.join(negative_REVIEWs_flattened)

# Creating the CountVectorizer
vectorizer = CountVectorizer(stop_words='english')

# Fitting and transforming the data
X13 = vectorizer.fit_transform([negative_text])

# Converting the result to a DataFrame
word_freq = pd.DataFrame(X13.toarray(), columns=vectorizer.get_feature_names_out())

# Getting the most common words
most_common_words = word_freq.sum().sort_values(ascending=False)[:20]

plt.figure(figsize=(10, 6))
most_common_words.plot(kind='bar', color='Red')
plt.title('Most Common Words in Negative REVIEWs')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

"""### <font color='Blue'> 9.4 2-gram most common words in reviews"""

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')
X = vectorizer.fit_transform(Rdata2['REVIEW'])
ngram_freq = pd.DataFrame.sparse.from_spmatrix(X, columns=vectorizer.get_feature_names_out())


most_common_ngrams = ngram_freq.sum().sort_values(ascending=False)[:20]

plt.figure(figsize=(12, 6))
most_common_ngrams.plot(kind='bar', color='pink')
plt.title('Most Common 2-grams in Reviews')
plt.xlabel('2-grams')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

negative_REVIEWs_ngram_freq = ngram_freq[Rdata['Sentiment'] == 3].sum()
positive_REVIEWs_ngram_freq = ngram_freq[Rdata['Sentiment'] < 3].sum()

# Plotting most common 2-grams for positive and negative Revi
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
negative_REVIEWs_ngram_freq.sort_values(ascending=False)[:10].plot(kind='bar', color='red')
plt.title('Most Common Negative 2-grams')
plt.xlabel('2-grams')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2)
positive_REVIEWs_ngram_freq.sort_values(ascending=False)[:10].plot(kind='bar', color='green')
plt.title('Most Common Positive 2-grams')
plt.xlabel('2-grams')
plt.ylabel('Frequency')
plt.figure(figsize=(12, 6))


plt.tight_layout()
plt.show()

plt.tight_layout()
plt.show()

